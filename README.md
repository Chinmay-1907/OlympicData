# 🏅 End-to-End Data Engineering Project: Olympic Data Analysis

Built an **end-to-end pipeline** using **Azure Data Factory** and **Databricks (Spark)** to integrate, transform, and analyze Olympic data. 
Perfomring **Extration** of data from an API, **Transforming** the data by using vairous data wrangling and cleaning methods, and lastly **Loading** the Data to a cloud database (Azure) Completing the full ETL Pipeline

---

## 📂 Dataset
- **Source**: Olympic Data via API

---

## 🛠 Tools and Frameworks
- **Tools**: Azure Data Factory, Databricks (Spark)
- **Platform**: Azure Data Lake Gen 2

---

## 🔍 Project Workflow
1. **API Integration**: Fetched Olympic data using API.
2. **ETL Processing**: Used Azure Data Factory for data extraction, transformation, and loading.
3. **Data Analysis**: Performed analysis in Databricks using Spark.

---

## 📊 Results
- **Transformed Data**: Stored in Data Lake Gen 2 for downstream analysis.
- **Insights**: Analyzed Olympic athlete performance and medal distributions.

---
